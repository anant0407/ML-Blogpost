{
  "hash": "1f7be36182fde60bf2e0e5570833c152",
  "result": {
    "markdown": "---\ntitle: \"A Beginner's Guide to Classification\"\ntitle-block-banner-color: white\nauthor: \"Anant Sharma\"\ndate: \"2023-11-28\"\ncategories: [news, code, analysis]\nimage: \"classification.svg\"\n---\n\n### Introduction:\n\nIn machine learning, classification reigns supreme as a powerful technique for predicting categorical outcomes. From spam filtering in emails to image recognition in autonomous vehicles, classification algorithms play a pivotal role in making sense of the world's data. This blog will unravel the intricacies of classification, exploring its principles, algorithms, and real-world applications.\n\n### Understanding Classification:\n\nClassification is a type of supervised learning where the algorithm learns from labeled training data to make predictions or decisions about new, unseen data. The goal is to categorize input data into predefined classes or labels based on their features.\n\n#### Key Concepts:\n\n1. **Features and Labels:**\n   - In a classification problem, features are the characteristics or variables that describe the data, while labels are the categories or classes that the algorithm aims to predict.\n\n2. **Training and Testing Data:**\n   - The dataset is typically divided into training and testing sets. The model learns patterns from the training data and is then evaluated on the testing data to assess its performance.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data\ndata, labels = make_blobs(n_samples=400, centers=4, random_state=18)\n\n# Visualize the generated data\nplt.scatter(data[:, 0], data[:, 1])\nplt.title('Generated Data')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=576 height=431}\n:::\n:::\n\n\n### Classification Algorithms:\n\n#### 1. **Logistic Regression:**\n   - Despite its name, logistic regression is a classification algorithm used for binary outcomes. It models the probability that a given input belongs to a particular class. Logistic Regression uses the sigmoid function to transform raw predictions into probabilities. The sigmoid function squashes values between 0 and 1.\n\n   The dataset is split into training and testing sets using train_test_split to train the model on one subset and evaluate its performance on another.\n\n\n   ::: {.cell execution_count=2}\n   ``` {.python .cell-code}\n   # Split the dataset into training and testing sets\n   X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n   ```\n   :::\n   \n   \n   We apply Logistic Regression using the LogisticRegression class from scikit-learn. The model is trained on the training set.\n\n\n   ::: {.cell execution_count=3}\n   ``` {.python .cell-code}\n   # Apply Logistic Regression\n   logreg = LogisticRegression(solver='liblinear')\n   logreg.fit(X_train, y_train)\n   ```\n   \n   ::: {.cell-output .cell-output-display execution_count=3}\n   ```{=html}\n   <style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>\n   ```\n   :::\n   :::\n   \n   \n   The accuracy of the model is evaluated on the test set, providing insights into its performance on unseen data.\n\n\n   ::: {.cell execution_count=4}\n   ``` {.python .cell-code}\n   # Evaluate the model on the test set\n   accuracy = logreg.score(X_test, y_test)\n   print(f\"Accuracy on the test set: {accuracy:.2%}\")\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Accuracy on the test set: 98.75%\n   ```\n   :::\n   :::\n   \n   \n\n   ::: {.cell execution_count=5}\n   ``` {.python .cell-code}\n   # Plot the decision boundary\n   plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30, cmap='tab10', marker='o', label='Actual')\n   plt.scatter(X_test[:, 0], X_test[:, 1], c=logreg.predict(X_test), s=10, cmap='viridis', marker='x', label='Predicted')\n   plt.title('Logistic Regression Classification Results')\n   plt.xlabel('Feature 1')\n   plt.ylabel('Feature 2')\n   \n   # Plot decision boundary\n   h = .02  # Step size in the mesh\n   x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n   y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n   xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n   Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n   Z = Z.reshape(xx.shape)\n   plt.contour(xx, yy, Z, colors='red', linewidths=1)\n   \n   plt.legend()\n   plt.show()\n   ```\n   \n   ::: {.cell-output .cell-output-display}\n   ![](index_files/figure-html/cell-6-output-1.png){width=587 height=449}\n   :::\n   :::\n   \n   \n   Logistic Regression is a versatile algorithm for binary classification tasks, and the sigmoid function enables it to output probabilities that can be thresholded to make predictions. Understanding how to apply Logistic Regression to real-world datasets, evaluate its performance, and interpret results is crucial for leveraging it effectively in classification scenarios.\n\n\n\n#### 2. **Decision Trees:**\n   - Decision Trees are powerful and interpretable machine learning models widely used for classification and regression tasks. Decision trees recursively split the data based on features, creating a tree-like structure of decisions that lead to the final classification. Let's delve into the principles behind Decision Trees, their construction, and how they make decisions based on input features.\n\n\n   ::: {.cell execution_count=6}\n   ``` {.python .cell-code}\n   X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n   \n   # Apply Decision Tree\n   decision_tree = DecisionTreeClassifier()\n   decision_tree.fit(X_train, y_train)\n   \n   # Evaluate the model on the test set\n   accuracy = decision_tree.score(X_test, y_test)\n   print(f\"Accuracy on the test set: {accuracy:.2%}\")\n   \n   # Plot the Decision Tree\n   plt.figure(figsize=(10, 6))\n   plot_tree(decision_tree, filled=False)\n   plt.title('Decision Tree for Classification')\n   plt.show()\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Accuracy on the test set: 98.75%\n   ```\n   :::\n   \n   ::: {.cell-output .cell-output-display}\n   ![](index_files/figure-html/cell-7-output-2.png){width=763 height=483}\n   :::\n   :::\n   \n   \n   Decision Trees provide a transparent and intuitive way to make predictions. They are particularly valuable for understanding the decision-making process in a machine learning model. Experimenting with different parameters and visualizing the tree structure enhances your grasp of Decision Trees and their applications in various domains.\n\n\n#### 3. **Random Forest:**\n   - Random Forest is a powerful ensemble learning technique that leverages the strength of multiple decision trees to improve classification performance. A collection of decision trees, random forests aggregate the predictions of individual trees to improve accuracy and robustness. Let's explore Random Forest\n\n\n   ::: {.cell execution_count=7}\n   ``` {.python .cell-code}\n   # Split the data into training and testing sets\n   X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n   \n   # Apply Random Forest Classifier\n   rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n   rf_classifier.fit(X_train, y_train)\n   \n   # Predictions on the test set\n   y_pred = rf_classifier.predict(X_test)\n   \n   # Calculate accuracy and display confusion matrix\n   accuracy = accuracy_score(y_test, y_pred)\n   conf_matrix = confusion_matrix(y_test, y_pred)\n   \n   print(f\"Accuracy: {accuracy * 100:.2f}%\")\n   print(\"Confusion Matrix:\")\n   print(conf_matrix)\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Accuracy: 98.75%\n   Confusion Matrix:\n   [[19  0  1  0]\n    [ 0 20  0  0]\n    [ 0  0 20  0]\n    [ 0  0  0 20]]\n   ```\n   :::\n   :::\n   \n   \n   Random Forest is a versatile and powerful algorithm for classification tasks, offering improved accuracy and robustness compared to individual decision trees. Experimenting with the number of trees (n_estimators) and other hyperparameters allows you to optimize the model for your specific dataset, making Random Forest a valuable tool for a wide range of classification problems.\n\n#### 4. **Support Vector Machines (SVM):**\n   - Support Vector Machines (SVM) are powerful and versatile machine learning models used for classification and regression tasks. SVM finds a hyperplane that best separates data into different classes, maximizing the margin between classes. Let's explore the fundamental concepts behind SVM and provide a practical example of using SVM for binary classification.\n\n\n   ::: {.cell execution_count=8}\n   ``` {.python .cell-code}\n   # Apply Support Vector Machine (SVM) Classifier\n   svm_classifier = SVC(kernel='linear', random_state=42)\n   svm_classifier.fit(X_train, y_train)\n   \n   # Predictions on the test set\n   y_pred = svm_classifier.predict(X_test)\n   \n   # Calculate accuracy and display confusion matrix\n   accuracy = accuracy_score(y_test, y_pred)\n   conf_matrix = confusion_matrix(y_test, y_pred)\n   \n   print(f\"Accuracy: {accuracy * 100:.2f}%\")\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Accuracy: 100.00%\n   ```\n   :::\n   :::\n   \n   \n   Support Vector Machines are effective for linear and non-linear classification tasks. The choice of the kernel function and other hyperparameters can significantly impact SVM's performance. Experimenting with different kernel functions and tuning parameters enhances your ability to leverage SVM for diverse classification problems.\n\n#### 5. **K-Nearest Neighbors (KNN):**\n   - K-Nearest Neighbors (KNN) is a simple yet powerful algorithm used for both classification and regression tasks. KNN classifies data points based on the majority class among their k nearest neighbors in the feature space. Let's explore the key concepts behind KNN and demonstrate its application for binary classification.\n\n\n   ::: {.cell execution_count=9}\n   ``` {.python .cell-code}\n   # Split the data into training and testing sets\n   X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n   \n   # Apply K-Nearest Neighbors (KNN) Classifier\n   knn_classifier = KNeighborsClassifier(n_neighbors=5)  # Set K to 5 (adjust based on dataset characteristics)\n   knn_classifier.fit(X_train, y_train)\n   \n   # Create a meshgrid to plot the decision boundaries\n   h = .02  # Step size in the mesh\n   x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n   y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n   xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n   \n   # Predict the class for each point in the meshgrid\n   Z = knn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n   Z = Z.reshape(xx.shape)\n   \n   # Plot the decision boundaries\n   plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.8)\n   \n   # Scatter plot of the data points\n   plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', s=30, edgecolors='k')\n   plt.title('KNN Classification with Decision Boundaries')\n   plt.xlabel('Feature 1')\n   plt.ylabel('Feature 2')\n   plt.show()\n   ```\n   \n   ::: {.cell-output .cell-output-display}\n   ![](index_files/figure-html/cell-10-output-1.png){width=587 height=449}\n   :::\n   :::\n   \n   \n   K-Nearest Neighbors is an intuitive algorithm that relies on the proximity of data points for classification. The choice of K and the distance metric significantly impact the model's effectiveness. Experimenting with different K values and understanding how it affects the balance between bias and variance is crucial for successfully applying KNN to various classification problems.\n\n### Real-World Applications:\n\n#### 1. **Image Recognition:**\n   - Classification is widely used in image recognition tasks, such as identifying objects, animals, or people in images.\n\n#### 2. **Spam Filtering:**\n   - In email systems, classification algorithms distinguish between spam and legitimate emails based on various features.\n\n#### 3. **Medical Diagnosis:**\n   - Classification is employed in healthcare to predict diseases or conditions based on patient data and medical history.\n\n#### 4. **Credit Scoring:**\n   - Financial institutions use classification models to assess the creditworthiness of individuals based on various financial factors.\n\n### Evaluation Metrics:\n\n1. **Accuracy:**\n   - The proportion of correctly classified instances out of the total instances.\n\n2. **Precision:**\n   - The ratio of true positives to the sum of true positives and false positives, emphasizing the accuracy of positive predictions.\n\n3. **Recall (Sensitivity):**\n   - The ratio of true positives to the sum of true positives and false negatives, emphasizing the coverage of actual positive instances.\n\n4. **F1 Score:**\n   - The harmonic mean of precision and recall, providing a balanced measure of a classifier's performance.\n\n### Challenges and Considerations:\n\n1. **Imbalanced Data:**\n   - Imbalanced class distributions can lead to biased models. Techniques like oversampling, undersampling, or using different evaluation metrics can help address this issue.\n\n2. **Feature Selection:**\n   - Choosing relevant features is crucial for the performance of a classification model. Feature engineering and selection techniques play a key role in this process.\n\n### Conclusion:\n\nAs we navigate the ever-expanding landscape of machine learning, the importance of classification algorithms in shaping our digital experiences cannot be overstated. Whether distinguishing between spam and legitimate emails or enabling self-driving cars to recognize pedestrians, the art of classification continues to empower machines to make informed decisions, transforming data into actionable insights in our technologically driven world.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}